# GAN to Discrete Event Simulator for MIDI Music Generation

Training of two GAN models on MIDI music data for the task of automated music composition. 

### Overview
1. [Requirements](#requirements)
2. [Demo](demo.ipynb)
2. [Folder Structure](#folder-structure)
3. [Meeting Protocol](#meeting-protocol)

# Requirements

## [requirements.txt](requirements.txt)

``pip install -r requirements.txt``

## [FluidSynth](https://www.fluidsynth.org/download/)

Install FluidSynth from the linked website. On Windows, FluidSynth has to be installed by using [Chocolatey](https://chocolatey.org/)
uing the following command:

``choco install fluidsynth``

Make sure that the path is stored in the system `PATH` variable and that the sound font file `FluidR3_GM.sf2` is present 
in each folder that you use FluidSynth in. 

FluidSynth is used to synthesize music from the generated MIDI files. We only use it for training the first model of the project (GAN-DES).

# [Demo](demo.ipynb)

Explore this Jupyter Notebook, for getting a quick overview of what models can be trained and how, as well as listening 
to some of their output.

# Folder Structure

## [Data](data)

This folder contains the data used for training. In our case, we trained on the [MAESTRO](https://magenta.tensorflow.org/datasets/maestro), which we stored in this folder. 
It is a dataset containing MIDI files with a size of 81 MB. The data contains 200 hours of virtuosic piano playing.

## [GAN DES](GAN_DES)

This folder contains the code to train and execute the Generative Adversial Network (GAN) to Discrete Event Simulator (DES).

### [adj_sim_outputs](GAN_DES/adj_sim_outputs)

The output files created during model training or when using the DES to generate music. Each format is stored, MIDI, spectrogram (.png), and audio (.wav).

### [logs](GAN_DES/logs)

The log file [simulation.log](GAN_DES/logs/simulation.log) is created by the DES and then used to generate the music.

### [models](GAN_DES/models)

Folder where the model files are stored during training. 

### [datasets](GAN_DES/datasets.py)

This python script contains the PyTorch Datasets for training on one song and training on the MAESTRO dataset. If you would
like to train on your own data, the data needs to be stored and loaded by implementing your own [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).

### [matrix_sim_process](GAN_DES/matrix_sim_process.py)

This python script contains the ``matrix_to_wav()`` function, which is used to convert the matrix generated by the generator
model to an audio wave. Then, this audio wave is converted to its spectrogram representation. 

### [sim_log_process_music.py](GAN_DES/sim_log_process_music.py)

This python file contains the ``process_adjsim_log()`` function, which is used by the ``matrix_to_wav()`` function for
generating music from the output of the DES.

### [SIMNN](GAN_DES/SIMNN.py)

This python script is used for training the GAN DES model. It contains the generator and discriminator models definitions, 
as well as the training hyperparameters. By running this script, the GAN DES model is trained.

### [simulation_v3](GAN_DES/simulation_v3.py)

Contains the actual DES. Is used by the function ``matrix_to_wav()`` to perform the simulation, after receiving the matrix
generated by the GAN generator. 

### [util](GAN_DES/util.py)

Contains different functions for creating the Mel Spectrogram and slicing audio.

## [MM GAN DES](MMGAN_MIDI_DES/network_tests.py) 

Code for training and using the multi-modal GAN (MM-GAN) with Discrete Event Simulator (DES).

This python script is used for training the MM-GAN DES model. It contains the generator, second beat generator 
and discriminator models definitions, as well as the training hyperparameters. 
By running this script, the MM-GAN DES model is trained. 

### [adj_sim_outputs](MMGAN_MIDI_DES//adj_sim_outputs)

The output files created during model training or when using the DES to generate music. The MIDI format is stored.

### [models](MMGAN_MIDI_DES//models)

Folder where the model files are stored during training. 

### [datasets](MMGAN_MIDI_DES/datasets.py)

This python script contains the PyTorch Datasets for training on the MAESTRO dataset, by pickling it and accessing it later for faster training.
It also contains the function ``generate_piano_roll()``, which generates piano rolls from MIDI data input. If you would
like to train on your own data, the data needs to be stored and loaded by implementing your own [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).

### [matrix_sim_process](MMGAN_MIDI_DES/matrix_sim_process.py)

This python script contains the ``matrix_to_midi()`` function, which is used to convert the matrix generated by the generator
model to a MIDI file directly.

### [sim_log_to_midi.py](MMGAN_MIDI_DES/sim_log_to_midi.py)

This python file contains the ``process_adjsim_log()`` function, which is used by the ``matrix_to_midi()`` function for
generating MIDI output from the output of the DES.

### [simulation_v3](MMGAN_MIDI_DES/simulation_v3.py)

Contains the actual DES. Is used by the function ``matrix_to_midi()`` to perform the simulation, after receiving the matrix
generated by the GAN generator. 

### [util](MMGAN_MIDI_DES/util.py)

Contains different functions for creating the Mel Spectrogram and slicing audio.

# Meeting Protocol

## 5th March

- Sadie set up BeatNet and All-in-One: BeatNet works well for extracting the up- and downbeat timestamps, all-in-one is able to detangle background, voice, drums, but not very well, only recognizes beats sometimes, does not really find segments
- could use Sadies simulation code for generating random combinations of a song
- only look at a short period of time of a song excerpt, for example 10 seconds of a 30 seconds excerpt, and compare the mutation of one of the song parts to another
- compare by computing a similarity measure
- for next time: Sadie set up evolution model, Marja set up model that creates a similarity measure

## 10th March 

- Sadie set up Discrete Event Simulator, output is good, but ranges of notes need to be improved
- goal for next meeting: Marja set up CNN for generator (generate input parameters for discrete event simulator) and discriminator (binary classifier for spectrogram of generated midi-file and original spectrogram)


## 13th March

- Marja started implementing the GAN structure according to this website: https://freedium.cfd/https://medium.com/geekculture/deep-convolutional-generative-adversarial-network-using-pytorch-ece1260acc47
- problem: how do we generate enough data to train on from just one song?
- pretrain on bigger data corpus, finetune on song data
- randomly manipulate song data, randomly add data from one song together to create more data, ...
- first only train on songs from one genre to have more data, then think about finetuning
- Sadie: create useful output from simulator for discriminator
- Marja: fix dimensions of GAN, input for simulator: nxn matrix, n=16, 16x20, values from 0-1

## 17th March

- Marja updated the output dimensions of GAN networks
- Sadie implemented midi to audio file converter and made changes to simulator
- Marja: randomly slice the input audio for more input clips of 5 seconds, requirements.txt
- Sadie: connect simulator to GAN

## 23rd March

- [ ] review feedback on project description: should we do a beat tracker evaluation, to have data, and see GAN training as extension?
- [ ] fix spectrogram dimensions of fake data: right now the fake data is too short, it should be 5*44100=220,500 samples long, but has dimensions (2, 88576)
- train on [piano dataset](https://magenta.tensorflow.org/datasets/maestro) (midi version) (simpler than gtzan because it does not have vocals)
- Marja write DataLoader for that
- if this is done set up input with beats (second discriminator)
- evaluation of BeatNet, All in One
- create 2 versions of the SimNN: without beat data (Marja) and with beat data (Sadie)
